import pandas as pd
import re
import os
import logging


# --- ChromaDB and Embedding Imports ---
import chromadb
import dotenv
# Use the embedding function provided by chromadb.utils
from chromadb.utils import embedding_functions

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('chroma_csv_loader')

# Load environment variables (make sure your .env file has GOOGLE_API_KEY)
dotenv.load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --- Configuration ---
# The CSV file generated by the processing pipeline
csv_file_path = "processed_output/chroma_prepared_final.csv" # Ensure this path is correct
# Directory where ChromaDB data will be stored
persist_directory = "./chroma_db_market_research"
# The name of the collection in ChromaDB
collection_name = "market_data_main"
# Column names in the CSV that correspond to ChromaDB fields
id_column = "chroma_id"
document_column = "document_text"

# --- Validate API Key ---
if not GOOGLE_API_KEY:
    logger.error("GOOGLE_API_KEY not found in environment variables. Please set it.")
    exit(1)
# Note: Verify if your FREE Gemini API key supports the `models/embedding-001` model.
# Embedding models usually have separate costs/tiers even if text generation is free.


# --- Load Data from CSV ---
try:
    # Use keep_default_na=False to prevent empty strings being read as NaN
    # dtype=str ensures all columns are read as strings initially, which simplifies
    # handling potential non-standard representations, then we convert known types.
    data = pd.read_csv(csv_file_path, header=0, keep_default_na=False, dtype=str)
    logger.info(f"Loaded {len(data)} rows from {csv_file_path}")
except FileNotFoundError:
    logger.error(f"CSV file not found: {csv_file_path}")
    exit(1)
except Exception as e:
    logger.error(f"Error loading CSV '{csv_file_path}': {e}")
    exit(1)

# --- Prepare Data for ChromaDB Format ---
if id_column not in data.columns or document_column not in data.columns:
    logger.error(f"CSV must contain '{id_column}' and '{document_column}' columns.")
    exit(1)

# Get IDs and Documents
# Ensure IDs are strings, as required by ChromaDB
chroma_ids = data[id_column].astype(str).tolist()
# Ensure documents are strings
chroma_documents = data[document_column].astype(str).tolist()

# Prepare Metadatas
metadata_columns = [col for col in data.columns if col not in [id_column, document_column]]
chroma_metadatas = []

logger.info("Preparing metadata for ChromaDB...")
for index, row in data.iterrows():
    meta = {}
    # Iterate through potential metadata columns
    for col in metadata_columns:
        value = row[col]

        # Skip empty strings or pandas NaN/None representations
        if isinstance(value, str) and not value.strip():
            continue
        if pd.isna(value): # Catches numpy.nan from default pandas loading if not using dtype=str
             continue

        # --- Metadata Value Conversion ---
        # Convert known string representations of numbers/booleans back to native types
        # This makes the metadata searchable by type in ChromaDB queries
        if isinstance(value, str):
            lower_value = value.lower()
            if lower_value == 'true':
                meta[col] = True
            elif lower_value == 'false':
                meta[col] = False
            # Attempt to convert to int or float if it looks like a number
            elif re.fullmatch(r'-?\d+', value): # Integer regex
                 try: meta[col] = int(value)
                 except (ValueError, OverflowError): meta[col] = value 
            elif re.fullmatch(r'-?\d+\.\d+(?:e-?\d+)?', value): # Float regex (simple)
                 try: meta[col] = float(value)
                 except ValueError: meta[col] = value 

            else:
                meta[col] = value

        else:
             # Fallback for any other unexpected types - convert to string
             meta[col] = str(value)
             logger.debug(f"Converted unexpected type for column '{col}' to string: {type(value)}")


    chroma_metadatas.append(meta)

# Final sanity check on list lengths
if not (len(chroma_ids) == len(chroma_documents) == len(chroma_metadatas)):
      logger.error(f"Length mismatch in prepared ChromaDB lists: IDs={len(chroma_ids)}, Docs={len(chroma_documents)}, Metadatas={len(chroma_metadatas)}. Exiting.")
      exit(1)

logger.info(f"Prepared {len(chroma_ids)} items for ChromaDB ingestion.")

# --- Initialize Embedding Function and ChromaDB Client ---
try:
    # Uses the embedding function provided by chromadb.utils
    # This requires the google-generativeai library to be installed
    google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
        api_key=GOOGLE_API_KEY,
        model_name="models/embedding-001" # Verify this model is available and supported by your key
    )
    # Use PersistentClient to store the database on disk
    client = chromadb.PersistentClient(path=persist_directory)
    logger.info(f"ChromaDB PersistentClient initialized at '{persist_directory}'.")
    logger.info(f"GoogleGenerativeAiEmbeddingFunction initialized for model '{google_ef.model_name}'.")

except Exception as e:
    logger.error(f"Error initializing embeddings function or ChromaDB client: {e}")
    logger.error("Please ensure 'chromadb', 'google-generativeai', and 'python-dotenv' are installed and GOOGLE_API_KEY is set correctly.")
    exit(1)

# --- Get or Create Collection and Add Data ---
logger.info(f"Attempting to get or create collection '{collection_name}'...")
try:
    # Get or create the collection, associating it with the embedding function
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=google_ef 
    )
    logger.info(f"Collection '{collection_name}' ready. It currently contains {collection.count()} documents.")

    # Add data in batches
    batch_size = 500 # You can adjust the batch size based on memory/API limits
    total_items = len(chroma_ids)

    logger.info(f"Adding/updating {total_items} documents to collection '{collection_name}' in batches of {batch_size}...")

    # Optional: Before adding, consider deleting documents if you are doing a full refresh
    # For example, get existing IDs and call collection.delete(ids=...)
    # This makes the add operation effectively an upsert if IDs are unique across runs.

    for i in range(0, total_items, batch_size):
        batch_end = min(i + batch_size, total_items)
        batch_ids = chroma_ids[i:batch_end]
        batch_docs = chroma_documents[i:batch_end]
        batch_metas = chroma_metadatas[i:batch_end]

        try:
            collection.add(
                ids=batch_ids,
                documents=batch_docs,
                metadatas=batch_metas
            )
            logger.info(f"Successfully added batch {i//batch_size + 1}/{(total_items + batch_size - 1)//batch_size} ({batch_end}/{total_items} items)")
        except Exception as batch_e:
            logger.error(f"Error adding batch starting at index {i}: {batch_e}", exc_info=True)


    logger.info(f"Finished adding/updating documents. Collection '{collection_name}' now contains {collection.count()} documents.")

except Exception as e:
    logger.error(f"An error occurred during ChromaDB collection handling or data addition: {e}", exc_info=True)
    exit(1)

logger.info("ChromaDB loading process completed.")