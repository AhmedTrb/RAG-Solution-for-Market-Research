import pandas as pd
import re
import os
import logging
import json # Needed for parsing and dumping JSON strings

# --- ChromaDB and Embedding Imports ---
import chromadb
import dotenv
# Use the embedding function provided by chromadb.utils
from chromadb.utils import embedding_functions
from typing import List # Needed for type hinting in the wrapper if used

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('chroma_csv_loader')

# Load environment variables (make sure your .env file has GOOGLE_API_KEY)
dotenv.load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --- Configuration ---
# The CSV file generated by the processing pipeline
csv_file_path = "processed_output/chroma_prepared_final.csv" 
# Directory where ChromaDB data will be stored
persist_directory = "../chroma_db_market_research"
# The name of the collection in ChromaDB
collection_name = "market_data_main"
# Column names in the CSV that correspond to ChromaDB fields
id_column = "chroma_id"
document_column = "document_text"

# --- Validate API Key ---
if not GOOGLE_API_KEY:
    logger.error("GOOGLE_API_KEY not found in environment variables. Please set it.")
    exit(1)

# --- Load Data from CSV ---
# Use keep_default_na=False to prevent empty strings from becoming NaN
# Using dtype=str to read all columns as strings initially is safer
try:
    data = pd.read_csv(csv_file_path, header=0, keep_default_na=False, dtype=str)
    logger.info(f"Loaded {len(data)} rows from {csv_file_path}")
except FileNotFoundError:
    logger.error(f"Error: CSV file not found at {csv_file_path}")
    exit(1)
except Exception as e:
    logger.error(f"Error loading CSV from {csv_file_path}: {e}")
    exit(1)


# --- Prepare Data for ChromaDB Format ---
if id_column not in data.columns or document_column not in data.columns:
    logger.error(f"CSV must contain '{id_column}' and '{document_column}' columns.")
    exit(1)

# Get IDs and Documents, ensuring they are strings
chroma_ids = data[id_column].astype(str).tolist()
chroma_documents = data[document_column].astype(str).tolist()

# Prepare Metadatas
metadata_columns = [col for col in data.columns if col not in [id_column, document_column]]
chroma_metadatas = []

logger.info("Preparing metadata for ChromaDB...")
for index, row in data.iterrows():
    meta = {}
    for col in metadata_columns:
        value = row[col]

        # Skip empty strings or pandas NaN representations
        if isinstance(value, str) and not value.strip():
            continue
        if pd.isna(value):
             continue

        # --- Metadata Value Conversion ---
        # Attempt to convert known string representations back to native types
        # This is important for ChromaDB's metadata filtering capabilities
        if isinstance(value, str):
            lower_value = value.lower()
            if lower_value == 'true':
                meta[col] = True
            elif lower_value == 'false':
                meta[col] = False
            # Attempt to convert to int or float
            elif re.fullmatch(r'-?\d+', value):
                 try: meta[col] = int(value)
                 except (ValueError, OverflowError): meta[col] = value # Keep as string if conversion fails
            elif re.fullmatch(r'-?\d+\.\d+(?:e-?\d+)?', value):
                 try: meta[col] = float(value)
                 except ValueError: meta[col] = value # Keep as string if conversion fails
            # Attempt to parse JSON strings back into lists/dicts
            # If parsing results in a list or dict, convert it BACK to a JSON string for ChromaDB
            elif value.startswith('[') or value.startswith('{'):
                 try:
                     parsed_value = json.loads(value)
                     # Check if the parsed value is a list or dict
                     if isinstance(parsed_value, (list, dict)):
                          # Convert it back to a JSON string for ChromaDB metadata
                          meta[col] = json.dumps(parsed_value)
                          # logger.debug(f"Converted JSON string for column '{col}' back to string for ChromaDB.") # Too verbose
                     else:
                          # If it was a JSON string for a primitive (like "123"), keep the parsed primitive
                          meta[col] = parsed_value
                          # logger.debug(f"Parsed JSON string for column '{col}' to primitive type {type(parsed_value)}.") # Too verbose
                 except json.JSONDecodeError:
                      # If it's not valid JSON, keep it as the original string
                      meta[col] = value
                      logger.debug(f"Column '{col}' looks like JSON but failed to parse. Keeping as string.")
            else:
                meta[col] = value # Keep as string if no specific conversion applies
        else:
             # Fallback for any other unexpected types - convert to string
             meta[col] = str(value)
             logger.debug(f"Converted unexpected type for column '{col}' to string: {type(value)}")

    chroma_metadatas.append(meta)

# Final sanity check on list lengths
if not (len(chroma_ids) == len(chroma_documents) == len(chroma_metadatas)):
      logger.error(f"Length mismatch in prepared ChromaDB lists: IDs={len(chroma_ids)}, Docs={len(chroma_documents)}, Metadatas={len(chroma_metadatas)}. Exiting.")
      exit(1)

logger.info(f"Prepared {len(chroma_ids)} items for ChromaDB ingestion.")

# --- Initialize Embedding Function and ChromaDB Client ---
try:
    google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
        api_key=GOOGLE_API_KEY,
        model_name="models/embedding-001" # Verify this model is available and supported by your key
    )
    # Use PersistentClient to store the database on disk
    client = chromadb.PersistentClient(path=persist_directory)
    logger.info(f"ChromaDB PersistentClient initialized at '{persist_directory}'.")
    logger.info(f"GoogleGenerativeAiEmbeddingFunction initialized for model '{google_ef.model_name}'.")

except Exception as e:
    logger.error(f"Error initializing embeddings function or ChromaDB client: {e}")
    logger.error("Please ensure 'chromadb', 'google-generativeai', and 'python-dotenv' are installed and GOOGLE_API_KEY is set correctly.")
    exit(1)

# --- Get or Create Collection and Add Data ---
logger.info(f"Attempting to get or create collection '{collection_name}'...")
try:
    # Get or create the collection, associating it with the embedding function
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=google_ef
    )
    logger.info(f"Collection '{collection_name}' ready. It currently contains {collection.count()} documents.")

    # Add data in batches
    batch_size = 500 # Adjust batch size based on memory/API limits
    total_items = len(chroma_ids)

    logger.info(f"Adding/updating {total_items} documents to collection '{collection_name}' in batches of {batch_size}...")

    # Optional: Consider deleting existing documents if doing a full refresh
    # collection.delete(ids=collection.get()['ids']) # Dangerous! Use with caution.

    for i in range(0, total_items, batch_size):
        batch_end = min(i + batch_size, total_items)
        batch_ids = chroma_ids[i:batch_end]
        batch_docs = chroma_documents[i:batch_end]
        batch_metas = chroma_metadatas[i:batch_end]

        # Use add or upsert. add will raise error if ID exists. upsert updates if ID exists, adds if not.
        # If your IDs are unique per run, add is fine. If you might re-process data, upsert is safer.
        # collection.add(ids=batch_ids, documents=batch_docs, metadatas=batch_metas) # Use add for unique IDs
        collection.upsert(ids=batch_ids, documents=batch_docs, metadatas=batch_metas) # Use upsert for robustness

        logger.info(f"Processed batch {i//batch_size + 1}/{(total_items + batch_size - 1)//batch_size} ({batch_end}/{total_items} items)")


    logger.info(f"Finished adding/updating documents. Collection '{collection_name}' now contains {collection.count()} documents.")

except Exception as e:
    logger.error(f"An error occurred during ChromaDB collection handling or data addition: {e}", exc_info=True)
    exit(1)

logger.info("ChromaDB loading process completed.")
